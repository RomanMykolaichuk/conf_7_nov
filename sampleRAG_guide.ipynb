{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Огляд використаних технологій\n",
    "\n",
    "Цей проєкт побудований на комплексному поєднанні технологій обробки природної мови, інформаційного пошуку та генерації відповідей, що дозволяє реалізувати підхід Retrieval-Augmented Generation (RAG). Нижче описано основні технології та концепції, які використовуються для досягнення результатів.\n",
    "\n",
    "#### 1. **Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "RAG – це технологія, яка комбінує витяг інформації (retrieval) з бази знань або документа та генерацію тексту за допомогою мовної моделі. Основні компоненти цього підходу:\n",
    "   - **Витяг інформації (retrieval)**: На основі індексованих документів здійснюється пошук релевантних сегментів тексту для відповіді на конкретний запит.\n",
    "   - **Генерація (generation)**: Мовна модель отримує контекст, отриманий із витягнутих даних, та генерує узгоджену відповідь на запит.\n",
    "\n",
    "RAG-підхід особливо корисний у сценаріях, де моделі потрібен додатковий контекст або доступ до специфічної інформації для формування точних відповідей.\n",
    "\n",
    "#### 2. **Мовна модель Llama 3.1 (Ollama)**\n",
    "\n",
    "Модель Llama 3.1, доступна через інтерфейс Ollama, забезпечує високу якість генерації тексту на основі запиту. Модель оптимізована для швидкої та точної обробки текстових запитів, що дозволяє генерувати детальні, узгоджені відповіді. Llama використовується для обробки розширених запитів (augmented prompts) і є ключовим елементом у RAG-системі.\n",
    "\n",
    "#### 3. **Векторне індексування (Vector Store Index)**\n",
    "\n",
    "Індекс векторного пошуку (Vector Store Index) створює векторні представлення текстових даних (ембеддінги), що дозволяє швидко знаходити релевантні фрагменти тексту на основі схожості. Основні компоненти:\n",
    "   - **Ембеддінги**: Числові представлення тексту, що дозволяють порівнювати різні фрагменти тексту.\n",
    "   - **Індексування та пошук**: Індексування дозволяє зберігати документи у векторному форматі, а засіб пошуку (retriever) швидко знаходить схожі фрагменти для заданого запиту.\n",
    "\n",
    "#### 4. **Hugging Face Transformers**\n",
    "\n",
    "Бібліотека Hugging Face Transformers забезпечує доступ до моделей і токенізаторів для створення ембеддінгів. У проєкті використовується модель `sentence-transformers/all-MiniLM-L6-v2` для генерації векторів тексту, які пізніше індексуються. Завдяки цій бібліотеці можна створювати локальні ембеддінги для тексту, що пришвидшує пошук і знижує потребу в зовнішньому інтернет-з’єднанні.\n",
    "\n",
    "#### 5. **Jupyter Notebook для інтерактивного тестування та налаштування**\n",
    "\n",
    "Jupyter Notebook використовується для інтерактивної розробки та тестування, дозволяючи виконувати код по клітинках і швидко отримувати зворотний зв’язок на різних етапах проєкту. Це зручне середовище для налаштування параметрів, аналізу результатів та демонстрації можливостей RAG-підходу.\n",
    "\n",
    "#### 6. **Змішані підходи до обробки тексту та керування середовищем**\n",
    "\n",
    "- **Python бібліотека `os`**: використовується для управління ключами доступу (API keys) та змінними середовища, що дозволяє безпечно зберігати конфіденційні дані.\n",
    "- **IPython.display**: використовується для виведення результатів у форматі Markdown, що покращує читабельність і наочність результатів, особливо в контексті взаємодії з Jupyter Notebook.\n",
    "\n",
    "### Висновок\n",
    "\n",
    "Поєднання RAG-підходу, мовної моделі Llama 3.1, векторного індексування та бібліотек для створення ембеддінгів забезпечує гнучку і потужну платформу для автоматизації пошуку та генерації текстових відповідей. Використання цих технологій дозволяє створити ефективну систему, здатну швидко знаходити релевантну інформацію та інтегрувати її у відповіді на складні запити, що особливо корисно в освітніх, дослідницьких та інформаційно-аналітичних проєктах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вступ\n",
    "\n",
    "Цей проєкт використовує методику Retrieval-Augmented Generation (RAG), яка комбінує вилучення інформації з наявних даних та генерацію нових текстових відповідей за допомогою мовних моделей. У цьому документі пояснюється, як завантажити дані, налаштувати середовище та виконати процес вилучення і генерації за допомогою моделі Llama 3.1.\n",
    "\n",
    "---\n",
    "\n",
    "### Markdown Cell 1: Встановлення необхідних бібліотек\n",
    "\n",
    "```python\n",
    "# Needed libraries installation (run it once)\n",
    "# %pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "Цей рядок коду використовується для встановлення всіх необхідних бібліотек, зазначених у файлі `requirements.txt`. Команда `%pip install` дозволяє швидко встановити залежності, потрібні для роботи з мовними моделями та іншими інструментами в цьому проєкті. Цей рядок закоментовано, але його можна розкоментувати для встановлення пакетів.\n",
    "\n",
    "---\n",
    "\n",
    "### Markdown Cell 2: Налаштування ключа API для OpenAI\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "Цей блок коду налаштовує змінну середовища `OPENAI_API_KEY`, що потрібна для роботи з OpenAI API. Вона задається через бібліотеку `os`, яка дозволяє керувати змінними середовища для зберігання конфіденційних даних у безпечному форматі. У цьому випадку, значення ключа API задано як `\"NA\"`, але на практиці воно повинно містити справжній ключ доступу до API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 3: Ініціалізація моделі Ollama\n",
    "\n",
    "```python\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", request_timeout=4000.0)\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **Імпорт Ollama** – з модуля `llama_index.llms.ollama` імпортується клас `Ollama`, що дозволяє використовувати модель Llama 3.1 для генерації тексту.\n",
    "2. **llm = Ollama(model=\"llama3.1\", request_timeout=4000.0)** – цей рядок коду створює об'єкт `llm`, що представляє модель Llama 3.1 з наступними параметрами:\n",
    "   - **model=\"llama3.1\"** – задає версію моделі для використання.\n",
    "   - **request_timeout=4000.0** – встановлює максимальний час очікування відповіді у 4000 секунд, що дозволяє уникнути переривання запиту через тривалий час обробки, особливо при обробці великих обсягів даних.\n",
    "\n",
    "Цей блок коду підключає модель Llama 3.1 для подальшого використання у процесі генерації тексту.\n",
    "\n",
    "---\n",
    "\n",
    "### Markdown Cell 4: Завантаження документів з директорії\n",
    "\n",
    "```python\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **Імпорт SimpleDirectoryReader** – клас `SimpleDirectoryReader` з `llama_index.core` дозволяє легко завантажувати документи з вказаної директорії.\n",
    "2. **documents = SimpleDirectoryReader(\"data\").load_data()** – цей рядок ініціалізує об'єкт `SimpleDirectoryReader` з параметром `\"data\"`, що вказує на папку з документами. Метод `load_data()` завантажує всі файли з цієї папки та зберігає їх у змінній `documents`.\n",
    "\n",
    "Цей блок коду забезпечує завантаження всіх документів з папки `data`, щоб модель могла використовувати їх для вилучення інформації під час генерації відповідей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 5: Налаштування моделі для локального збереження ембеддінгів\n",
    "\n",
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "save_directory = \"./local_embedding_model\"\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **Імпорт AutoModel і AutoTokenizer** – ці класи з бібліотеки `transformers` дозволяють працювати з моделями та токенізаторами для створення ембеддінгів тексту. Ембеддінги – це числові представлення текстових даних, які можуть бути корисними для подальшого аналізу або пошуку інформації.\n",
    "2. **save_directory = \"./local_embedding_model\"** – змінна `save_directory` вказує на директорію для збереження локальної копії моделі ембеддінгів. Це дозволяє використовувати збережені дані локально без необхідності повторного завантаження з інтернету, що оптимізує швидкість роботи.\n",
    "\n",
    "Цей блок коду підготовлює директорію для збереження моделі ембеддінгів, яка буде використовуватися для обробки текстових даних та вилучення інформації.\n",
    "\n",
    "---\n",
    "\n",
    "### Markdown Cell 6: Збереження моделі ембеддінгів локально\n",
    "\n",
    "```python\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **Завантаження моделі та токенізатора** – `AutoModel.from_pretrained` і `AutoTokenizer.from_pretrained` завантажують модель та токенізатор для ембеддінгів з репозиторію `sentence-transformers/all-MiniLM-L6-v2`. Ця модель спеціалізується на перетворенні тексту в ембеддінги, що придатні для пошукових і класифікаційних задач.\n",
    "2. **Збереження моделі та токенізатора локально**:\n",
    "   - `model.save_pretrained(save_directory)` зберігає модель у вказаній директорії.\n",
    "   - `tokenizer.save_pretrained(save_directory)` зберігає токенізатор у тій самій директорії.\n",
    "\n",
    "Цей код зберігає модель та її токенізатор локально, що дозволяє уникнути повторного завантаження та оптимізує час виконання під час майбутніх запитів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 7: Створення функції для ембеддінгів\n",
    "\n",
    "Цей блок коду створює спеціалізовану функцію для генерації ембеддінгів тексту з використанням налаштованої моделі та токенізатора. Це дозволяє нам перетворити текст на числові представлення, які можна використовувати для пошуку та класифікації інформації.\n",
    "\n",
    "```python\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "import torch\n",
    "\n",
    "class LocalEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self._model = model  # Приватний атрибут для зберігання моделі\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def get_text_embedding(self, text):\n",
    "        # Токенізація з обрізанням до максимального розміру моделі\n",
    "        inputs = self._tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "        # Обчислення середнього значення останнього прихованого стану як ембеддінгу\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        return embedding\n",
    "\n",
    "    # Реалізація необхідних методів\n",
    "    def _get_query_embedding(self, query):\n",
    "        return self.get_text_embedding(query)\n",
    "\n",
    "    def _get_text_embedding(self, text):\n",
    "        return self.get_text_embedding(text)\n",
    "\n",
    "    async def _aget_query_embedding(self, query):\n",
    "        return self.get_text_embedding(query)\n",
    "\n",
    "# Ініціалізація користувацької функції для ембеддінгів\n",
    "embedding_model = LocalEmbedding(model=model, tokenizer=tokenizer)\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **Клас `LocalEmbedding`** – створює новий клас для роботи з ембеддінгами, який наслідує від `BaseEmbedding`. Він включає методи для обробки текстових запитів та перетворення їх на ембеддінги.\n",
    "2. **Метод `get_text_embedding`**:\n",
    "   - Виконує токенізацію тексту з використанням обмеження до 512 токенів.\n",
    "   - За допомогою `torch.no_grad()` генерує вихідні значення, не зберігаючи обчислення для градієнтів, що знижує обсяг ресурсів.\n",
    "   - Обчислює середнє значення останнього прихованого стану як вектор ембеддінгу.\n",
    "3. **Додаткові методи**:\n",
    "   - `_get_query_embedding` та `_get_text_embedding` використовують `get_text_embedding` для отримання ембеддінгу запиту чи тексту.\n",
    "   - `_aget_query_embedding` – асинхронна версія, яка дозволяє інтегрувати цю функцію в асинхронні процеси.\n",
    "\n",
    "Цей клас забезпечує зручний спосіб створювати та обробляти ембеддінги тексту, що важливо для реалізації функцій пошуку та вилучення інформації в RAG-системах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 8: Налаштування та створення індексу VectorStore для роботи з документами\n",
    "\n",
    "Цей блок коду налаштовує глобальні параметри для роботи з індексом векторного пошуку, який використовує модель Llama та спеціалізовану функцію ембеддінгів. Індекс векторного пошуку дозволяє ефективно знаходити релевантні документи на основі схожості їх векторних представлень.\n",
    "\n",
    "```python\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "\n",
    "# Налаштування глобальних параметрів\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "# Встановлення розміру сегмента (кількість токенів у сегменті)\n",
    "Settings.chunk_size = 256\n",
    "# Встановлення перекриття сегментів (кількість токенів, що перекриваються між сегментами)\n",
    "Settings.chunk_overlap = 64\n",
    "\n",
    "# Створення індексу на основі завантажених документів\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **Налаштування глобальних параметрів (`Settings`)**:\n",
    "   - `Settings.llm = llm` – вказує, яка мовна модель (Llama 3.1) буде використовуватися для генерації відповідей.\n",
    "   - `Settings.embed_model = embedding_model` – встановлює модель ембеддінгів (`embedding_model`), яка буде перетворювати текст у векторні представлення.\n",
    "   - `Settings.chunk_size = 256` – визначає кількість токенів у кожному сегменті документа. Це розбиває документ на невеликі частини, полегшуючи пошук.\n",
    "   - `Settings.chunk_overlap = 64` – встановлює кількість токенів, що перекриваються між сегментами. Це забезпечує плавність переходу між сегментами та знижує ймовірність втрати важливої інформації на межі сегментів.\n",
    "   \n",
    "2. **Створення індексу**:\n",
    "   - `VectorStoreIndex.from_documents(documents, show_progress=True)` створює індекс, використовуючи завантажені документи, та показує прогрес створення. Індекс дозволяє виконувати ефективний пошук за векторними представленнями тексту.\n",
    "\n",
    "Цей блок коду забезпечує підготовку документів до пошуку, налаштовуючи параметри для оптимальної роботи з текстовими даними та генерацією ембеддінгів. Така структура індексу підходить для пошуку релевантних сегментів тексту в системах, що використовують RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 9: Налаштування засобу пошуку (retriever) з використанням індексу\n",
    "\n",
    "Цей блок коду налаштовує об'єкт `retriever` для виконання пошуку релевантних сегментів тексту з індексу. Це дозволяє знайти документи або їх частини, які найбільше відповідають заданому запиту.\n",
    "\n",
    "```python\n",
    "# Налаштування засобу пошуку з використанням індексу\n",
    "top_k = 5\n",
    "retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **top_k** – змінна, що визначає кількість найбільш релевантних результатів, які буде повертати засіб пошуку. У цьому випадку значення `top_k = 5` означає, що буде знайдено та повернено до 5 найбільш схожих сегментів.\n",
    "2. **index.as_retriever(similarity_top_k=top_k)** – цей метод створює об'єкт `retriever` з індексу. `similarity_top_k=top_k` визначає, що пошук виконується на основі схожості та повертає до `top_k` найближчих відповідей.\n",
    "\n",
    "Цей `retriever` дозволяє виконувати швидкий пошук релевантної інформації в індексі, повертаючи найбільш підходящі результати на запити користувача. Це важлива частина системи RAG, яка забезпечує доступ до релевантних даних для подальшої генерації текстових відповідей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 10: Функція для створення розширеного запиту з контекстом\n",
    "\n",
    "Ця функція `get_augmented_prompt` створює розширений запит (prompt) для генерації відповіді, який включає витягнуті сегменти тексту з індексу. Це підхід Retrieval-Augmented Generation (RAG), де контекст додається до запиту для підвищення точності відповіді.\n",
    "\n",
    "```python\n",
    "def get_augmented_prompt(query):\n",
    "    retrieved_documents = retriever.retrieve(query)\n",
    "    # Доповнюємо запит, включаючи витягнуті документи\n",
    "    # Створення розширеного запиту з контекстом із знайдених документів\n",
    "    augmented_prompt = f\"Context:\\n\"\n",
    "    for index, doc in enumerate(retrieved_documents):\n",
    "        text = doc.text\n",
    "        print(f\"Retrieved {index+1}: {text} \\n\")\n",
    "        augmented_prompt += f\"{text}\\n\"\n",
    "    augmented_prompt += f\"\\nQuestion: {query}\\nPlease provide a concise and accurate answer based on the context.\"\n",
    "    return augmented_prompt\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **retriever.retrieve(query)** – виконує пошук в індексі за запитом `query` і повертає релевантні документи.\n",
    "2. **augmented_prompt** – змінна, що формується для створення повного запиту з контекстом. Вона починається з тексту `\"Context:\\n\"`, після якого додається вміст кожного витягнутого документа.\n",
    "3. **for loop** – цикл, що проходить по кожному витягнутому документу. Для кожного документа:\n",
    "   - **print(f\"Retrieved {index+1}: {text} \\n\")** – виводить на екран текст витягнутого сегмента з його індексом.\n",
    "   - **augmented_prompt += f\"{text}\\n\"** – додає текст витягнутого документа до розширеного запиту.\n",
    "4. **Додавання питання** – в кінці `augmented_prompt` додається текст запиту та інструкція для створення точної відповіді на основі контексту.\n",
    "\n",
    "Ця функція створює розширений запит, який містить релевантні витяги з документів, що надає додатковий контекст для точного формулювання відповіді на питання. Такий підхід є основою RAG-систем, де модель отримує додаткову інформацію перед створенням відповіді."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cell 11: Отримання відповіді з використанням RAG-підходу\n",
    "\n",
    "Ця функція `get_RAG_response` надсилає розширений запит із доданим контекстом до мовної моделі для генерації узгодженої відповіді на основі отриманої інформації.\n",
    "\n",
    "```python\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Надсилання розширеного запиту до мовної моделі для отримання узагальненої відповіді\n",
    "def get_RAG_response(query):    \n",
    "    response = llm.complete(get_augmented_prompt(query))\n",
    "    return response\n",
    "```\n",
    "\n",
    "#### Пояснення\n",
    "\n",
    "1. **get_augmented_prompt(query)** – викликає функцію `get_augmented_prompt`, яка створює розширений запит із контекстом, доданим з витягнутих документів, щоб забезпечити точність відповіді.\n",
    "2. **llm.complete(augmented_prompt)** – надсилає розширений запит до мовної моделі (налаштованої як `llm`). Модель обробляє запит з контекстом і генерує відповідь, яка базується на наданій інформації.\n",
    "3. **return response** – повертає згенеровану моделью відповідь, що є результатом обробки запиту з урахуванням контексту.\n",
    "\n",
    "Функція `get_RAG_response` використовує RAG-підхід, комбінуючи витягнуті релевантні дані з індексу та мовну модель для створення детальної відповіді. Цей підхід допомагає моделі генерувати більш точні та релевантні відповіді на запити, особливо коли потрібен додатковий контекст із документів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загальні висновки\n",
    "\n",
    "Цей проєкт демонструє, як сучасні технології обробки природної мови можуть значно покращити процес пошуку інформації та генерації тексту на основі запитів. Використовуючи підхід Retrieval-Augmented Generation (RAG), система дозволяє отримувати високоякісні, контекстно-орієнтовані відповіді, об’єднуючи переваги інформаційного пошуку та генерації тексту. Ось основні висновки:\n",
    "\n",
    "1. **Ефективне поєднання пошуку та генерації тексту**:\n",
    "   - RAG забезпечує точні відповіді на основі релевантного контексту, який система знаходить у попередньо завантажених документах. Це значно покращує якість відповідей порівняно зі звичайною генерацією без контексту, особливо для складних запитів, які потребують детальної інформації.\n",
    "\n",
    "2. **Використання векторного індексування для швидкого доступу до інформації**:\n",
    "   - Завдяки індексу векторного пошуку проєкт може швидко знаходити найбільш релевантні документи або їх частини, що значно підвищує швидкість обробки запитів і точність відповіді. Векторне індексування та пошук по ембеддінгам забезпечують гнучкість і масштабованість для великих обсягів тексту.\n",
    "\n",
    "3. **Застосування мовної моделі Llama 3.1 для генерації якісних текстів**:\n",
    "   - Модель Llama 3.1 демонструє високу якість генерації відповідей завдяки можливості працювати з контекстом і складними запитами. Це відкриває нові можливості для використання мовних моделей в інформаційно-аналітичних та освітніх проєктах.\n",
    "\n",
    "4. **Гнучкість і масштабованість рішення**:\n",
    "   - Використання RAG-підходу дозволяє адаптувати систему під різні потреби, зокрема, в освітній сфері, дослідницьких проєктах, аналітиці та інших галузях. Система легко масштабується і може працювати з великими наборами даних, завдяки чому її можна адаптувати для широкого спектру завдань.\n",
    "\n",
    "5. **Безпечне та надійне керування конфіденційними даними**:\n",
    "   - Проєкт демонструє практику безпечного зберігання ключів API та налаштувань середовища, що є критично важливим аспектом при роботі з хмарними платформами та конфіденційними даними. \n",
    "\n",
    "### Підсумок\n",
    "\n",
    "Цей проєкт є прикладом того, як сучасні методи обробки природної мови можуть значно покращити роботу з інформацією, забезпечуючи швидкий доступ до потрібних даних та точну генерацію відповідей на запити. Поєднання RAG, векторного індексування та мовних моделей створює гнучкий інструмент, який може бути корисним у різноманітних застосуваннях — від освіти до аналізу даних."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
